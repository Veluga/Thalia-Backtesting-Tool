\documentclass[main.tex]{subfiles}

\begin{document}

\section{Design}
\label{Design}

\subsection{General Design Decisions}
We have chosen pandas dataframes \cite{pandas} as the common data format for both data exchange and any calculations. Pandas provides us with data structures ``that cohere with the rest of the scientific Python stack`` \cite{mckinney2011pandas}, such as NumPy, which we are using to calculate historical returns and risk metrics (\ref{BL Structure}). Additionally, it is supported natively by many third party APIs and can even be used to read data from SQL databases. For additional information, please consult the official pandas documentation \cite{pandas}.

\subsection{Overview of System Architecture}

Before discussing any specific component and architectural layer in depth, it is worth revisiting our original architecture \cite{TR}.
\begin{figure}[H]
	\includegraphics[width=\textwidth]{04Design/04Pictures/architecture_layer_diagram.png}
    \caption{Original Thalia System Architecture \cite{TR}}
\end{figure}
The above schematic illustrates how we modified a typical Three Layer architecture to include a Data Collection Layer. Subsequent discussion will refer to this layer as the `Data Harvester`. It consists of adapters for third party APIs which offer price data for financial assets. The associated API will be queried according to a configurable interval to allow for live updates to our price data. Additionally, initial seeding of the database with historical price data can also be achieved via the Data Harvester. The decision to isolate this system component has been made in order to increase security by limiting the access of the Thalia web service to the financial data database to read-only and to allow for independent scaling of the Data Harvester and our web service \cite{TR}.

While the architecture of our system has stayed the same, there have been some modifications to individual components. Most notable are the changes to the Database Structure examined in \ref{DB Structure}. The following sections will provide for a discussion of design decisions made on a layer-by-layer basis.

\subsection{GUI Structure}

The Graphical User Interface of Thalia consists of two main parts. One is a \emph{static} website created with HTML5 \cite{html5} and stylised with the help of Bulma \cite{bulma}. The second is a \emph{dynamic} Dash \cite{dash} page, which is basically the bulk of our application. In the following paragraphs we will discuss these separately. 

\subsubsection{Thalia Web}

Thalia Web consists of 5 pages, which all serve as an introduction to our application. We aimed at creating an authentic and modern looking website that attracts users. This was done by following a consistent style for the whole website, i.e. using the official colours, motto and Thalia logo, as discussed in \ref{Branding}.

While Thalia was not meant to be used on smaller devices due to the nature of our application, with the help of Bulma, every component of Thalia Web is responsive. Consequently, the layout of the website changes so that it fits the browser size perfectly.

Even though it is possible to navigate between pages by URLs directly, we have designed a navigation bar for this purpose. The look of this navigation bar depends on the device of the current user and whether the user has already been logged in or not. By default, an unauthenticated user will see the navigation bar visible on \figurename{\ref{thalia_navbar_design}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/navbar.png}
   \caption{Thalia Navigation Bar}
   \label{thalia_navbar_design}
\end{figure}

In case Thalia is launched on a mobile phone, the navigation bar becomes a so-called `hamburger button` and dropdown menu visible on \figurename{\ref{thalia_navbar_hamburger_design}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/navbar_hamburger.png}
   \caption{Thalia Navigation Bar - Hamburger}
   \label{thalia_navbar_hamburger_design}
\end{figure}

For the rest of this section we shall assume that the application was launched on a desktop, although the layout is identical and intuitive in the mobile case. 

\subsubsection*{Homepage}

We shall not discuss every page separately, as they are identical. For a full walkthrough, please refer to  \ref{user_manual}. As an example of our design choices, let us look at the homepage where users should arrive by default.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/homepage.png}
   \caption{Thalia Homepage (source: http://thaliabacktest.xyz/)}
   \label{thalia_home_design}
\end{figure}

As users arrive on the homepage, they are greeted with a short description of Thalia and the process of backtesting. We have designed the homepage such that the navigation bar and the Thalia Logo fills the screen completely. To indicate that the page does not end there, we have added a scroll down button, which with the help of a JavaScript \cite{js} animation takes the user down to the bottom of the page. Here, the user may encounter a small register form, or in case the user is logged in, a link to the dashboard, i.e. the main application.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/homepage_bottom.png}
   \caption{Top: User not logged in; Bottom: User recognised}
   \label{thalia_home_bottom_design}
\end{figure}

\subsubsection*{Log In and Sign Up Pages}

In case the user is not yet logged in, links for the `Log In` and `Sign Up` pages are visible on the navigation bar as seen on \figurename{\ref{thalia_navbar_design}} or \figurename{\ref{thalia_navbar_hamburger_design}}.
Both of these forms are quite common, with the login requiring:

\begin{itemize}
    \item Username
    \item Password
    \item (Optional) Remember me
\end{itemize}

And for signing up, the fields are:

\begin{itemize}
    \item Username
    \item Password
    \item Confirm Password
\end{itemize}

As standard, the registration fails when the user enters different values to the `Password` and `Confirm Password` fields. In this case, the user is prompted to try again.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/login.png}
   \caption{Thalia Log In Page (source: http://thaliabacktest.xyz)}
   \label{thalia_login_design}
\end{figure}

In case the user is already logged in and attempts to access these pages, they will be redirected to the homepage. Additionally, the navigation bar changes, allowing us to log out.

\subsubsection{The Dashboard}

\say{Dash apps are composed of two parts. The first part is the `layout` of the app and it describes what the application looks like. The second part describes the interactivity of the application [...]} \cite{dash_layout}. Creating the Dashboard with Dash meant using Python classes for all of the visual components of the application.

Writing code for the layout was intuitive, but cumbersome, as on top of the HTML-like structure, it also needed to be correctly indented. Consequently, we needed to rely on refactoring a lot, chopping the layout code into smaller, more manageable components. In our final design, we have decided to divide the Dashboard into tabs, which helped us achieve a more intuitive look and organised codebase. The layout of the dashboard can now be initialised as follows:

\begin{lstlisting}[language=Python, caption=layout.py - Example of Dash Code, label=lst:dash_code]
import dash_core_components as dcc
import dash_html_components as html
from . import tabs

layout = html.Div(
    html.Main(
        [
            html.Div(
                [
                    dcc.Tabs(
                        [
                            tabs.tickers(),
                            tabs.summary(),
                            tabs.metrics(),
                            tabs.returns(),
                            tabs.drawdowns(),
                            tabs.assets(),
                        ],
                        id="tabs",
                        value="tickers",
                    ),
                ],
                className="column",
            ),
        ],
        className="columns",
    ),
    className="section",
)
\end{lstlisting}

Upon arriving at the Dashboard, only the `Ticker Selection` tab is available to the user. This is where the user is expected to input their investment strategy. Although this process is fairly intuitive and done by using dropdown menus, input fields and standard date selectors, a more thorough walkthrough can be found in \ref{user_manual}. 

For the selection of assets we have decided to take an alternative approach visible on \figurename{\ref{thalia_table_design}}. This was done mostly because of the limitations of Dash discussed in \ref{dash_limitations}, but came with its own benefits.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/table.png}
   \caption{Thalia Dashboard - Assets Table}
   \label{thalia_table_design}
\end{figure}

The official documentation describes the dash table as \say{an interactive table component designed for viewing, editing, and exploring large datasets} \cite{dash_table}. Working with the dash table is also quite simple, as its data can be accessed by addressing the id of the table and then the data component. For a further explanation on registering Dash Components, read \ref{GUI}.

Accessing the data like this also allowed us to fulfill one of our requirements, which is to support portfolios with a large number of assets. This would have been significantly harder, if not impossible, with alternative approaches, see \ref{dash_limitations}.

To compare investment strategies, the user may add portfolios via the `Add Portfolio` button. Due to the arguments presented in \ref{dash_limitations}, we have decided to set the maximum number of portfolios to 5, compared to letting it be dynamic. In addition, it is possible to select from a set of benchmarks, also known as lazy portfolios. Having selected one, the user will find the table populated with the desired assets and proportions. 

When the user is content with the input, they can start backtesting by clicking the `Submit` button. If all required fields are populated, the user is taken to the summary tab. This, as well as all other tabs, is now unlocked.

\subsubsection*{Showing the Results}

Many studies have shown that one most important factor when designing a dashboard application is to show the right data using the right visualization tools \cite{dashboard_design1} \cite{dashboard_design2}. In our case, we have already established which metrics and plots are most important to the users. When the user lands on the summary tab, it is these key metrics that are presented, leaving a more detailed analysis for the relevant tabs. This way the user is not overloaded with information as soon as the results are presented. 

Having decided on `What` the user should see we can now focus on the `How`. In our final design, we combine data visualization techniques to make the dashboard look interesting. Key metrics are shown either in a `box`, which is a core building block of a dashboard or in tables. Proportions are visualised using pie charts, whereas relative differences are shown with the help of a bar chart. Some examples can be seen on \figurename{\ref{dashboard}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{04Design/04Pictures/dashboard.png}
   \caption{The Dashboard}
   \label{dashboard}
\end{figure}

Among these charts, the user can find one of the key components of our application, the Portfolio Growth Plot. This graph is crucial for our application, as it serves as a visualisation for two of our main functional requirements, i.e. showing the total returns of a portfolio over time and the comparison of strategies.
Thanks to Dash, all graphs are fully interactive. The user may zoom in on selected areas, hover over desired data points, save the plot as an image, etc.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{08Appendices/081User/081Pictures/dash_funcionalities.png}
   \caption{Dash Functionalities}
   \label{dash_functionalities_design}
\end{figure}

The result of performing one such action on the Portfolio Growth Plot can be seen on \figurename{\ref{portfolio_growth}}. In this case, the user is about to enlarge the selected region to inspect the plot closer. Resetting the graph can then be done by one of the buttons visible on \figurename{\ref{dash_functionalities_design}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{04Design/04Pictures/portfolio_growth.png}
   \caption{Dash Functionalities - Portfolio Growth}
   \label{portfolio_growth}
\end{figure}

\subsubsection{Changes to the Initial Design}

The design of the Dashboard was done according to the wireframes we have provided last term \cite{TR}, also visible on \figurename{\ref{wireframe}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{04Design/04Pictures/wireframe.png}
   \caption{UI Wireframes}
   \label{wireframe}
\end{figure}

As we can see, we managed to encapture the overall look as we envisioned with only minor changes. The data is significantly better distributed by the use of tabs, giving a less cluttered UI. Additionally, we have decided not to implement the aside menu, partly because it was unnecessary, but mostly because it suggests that the backtest results can be changed on the go. This, as we have realised, was not feasible, due to expensive calculations in the Business Logic Layer.

One change we had to make was the use of the Dash table instead of the approach visible on \figurename{\ref{wireframe}}. The benefits of this change have been illustrated further above. In addition, we have also decided to discard the sliders as a way of changing asset allocations.

For portfolio comparison, we also had to enable inputting multiple strategies. As seen on \figurename{\ref{wireframe}}, this is something we have not accounted for when planning the design. The implementation of this feature led to a full refactoring of the UI code, as it came with an excessive amount of code due to the arguments presented in \ref{dash_limitations}. Fortunately, this issue was addressed relatively early and resulted in the current design. 

\subsection{Business Logic Structure}
\label{BL Structure}

The responsibility of our business logic can be summarised as follows: Given an investment strategy specification input from a user via the GUI, retrieve relevant financial data from the database to perform calculations for the historical performance and associated risk metrics.

To achieve this, we have developed a library (Anda \ref{project_glossary}) that performs the necessary calculations. Anda is decoupled from both the presentation and database layer by relying on external providers for any price data and the specification of a strategy.
This decision has been made to allow for alternative sources of price data. One of our features is allowing users to input their own price data for assets not supported by Thalia. This data is uploaded as a CSV file. Without our current design, i.e. by coupling calculation of performance and metrics to database access, we would have to modify the business logic to support multiple data sources. Given our current implementation, however, we can simply parse the user data into a pandas dataframe in a wrapper around Anda and then call functions within the library as required.
Currently supported metrics include `Total Return`, `Max Drawdown`, `Best / Worst Year`, and the `Sharpe` and `Sortino` Ratios (please refer to \ref{domain_glossary} for definitions). However, the library is open for extension, hence additional metrics may be added at any point.

For a closer look at how an investment strategy is specified, consider the following class that serves as input to Anda library functionality (e.g. for calculating the Sharpe Ratio):

\begin{lstlisting}[language=Python, caption=Strategy Object Specification]
import pandas as pd

class Strategy:
    def __init__(
        self,
        start_date: date,
        end_date: date,
        starting_balance: Decimal,
        assets: [Asset],
        contribution_dates,  # implements __contains__ for date
        contribution_amount: Decimal,
        rebalancing_dates,  # implements __contains__ for date
    ):
        self.dates = pd.date_range(start_date, end_date, freq="D")
        self.starting_balance = starting_balance
        self.assets = assets
        self.contribution_dates = contribution_dates
        self.contribution_amount = contribution_amount
        self.rebalancing_dates = rebalancing_dates
\end{lstlisting}

Here, `Asset` is a simple dataclass consisting of a ticker string (e.g. `MSFT` for Microsoft), a weight as a share of the portfolio overall (e.g. 0.25), a pandas dataframe holding historical price data ordered by date, and a pandas dataframe for dividends data (if any). An overview over the class relationships can be found in \ref{umls}.

As alluded to earlier, functions within the library depend on a Strategy object for their calculations. For example:

\begin{lstlisting}[language=Python, caption=Example Function Signature]
def total_return(strat) -> pd.Series:
\end{lstlisting}
will calculate a series of total return values ordered by date within the date range specified in the passed Strategy instance.

Another important design decision has been the choice of data type to represent money, for example for price data. For this, we have chosen the Decimal type from the Python Standard Library `decimal` module, since it ``provides support for fast correctly-rounded decimal floating point arithmetic`` \cite{PyDecimal}. As rounding errors and imprecision are unacceptable for our application, using the Decimal type will allow us to reliably compute figures for prices, risk metrics, etc.

Finally, we have chosen NumPy \cite{walt2011numpy} for performing numerical calculations as this allows for highly optimized computation through the use of vectorized operations.

\subsection{Data Harvester Structure}

The role of the Data Harvester is the collection of live data from multiple source, parsing it to a format that makes it usable by Anda and easily storable by our in house DBMS system Finda. In addition to this, a mechanism for changing the data collected and adapting to the market demands has to exist. Another requirement relates to the limitations that are imposed by the third-party APIs we use. The first design decision made in order to accomplish the above was the creation of the updating mechanism.

All financial backtesting applications rely on having accurate data to work with. We required a system that can aggregate the data from multiple APIs and that would keep our database updated with live data.
The requirements for such a system were:

\begin{itemize}
    \item \textbf{Standard data format:} To be able to store the data into a database with Finda and have the financial analysis done by Anda, the Harvester needs to parse all the data received through the APIs to a standard format without losing numerical accuracy or granularity.
    \item \textbf{Update the database:} The database has to be updated regularly to contain live data.
    \item \textbf{Data Interpolation:} Some financial assets require interpolation to account for days without any transactions (e.g. weekends and public holidays).
    \item \textbf{Redundancy:} We have no power over the APIs we are using. The required system needs to have redundancy measures so that we can retrieve live data even if an API fails.
    \item \textbf{Maintenance:} The financial world is in a continuous change. Because of this, it is important to make sure that we can add and delete assets as time goes on.
\end{itemize}

\subsubsection{Continuous Updating Mechanism}
The Updating consists of three components:
\begin{itemize}
    \item \textbf{The update procedure} made out of a series of methods. The procedure starts by looking at what APIs are available to be called. Each API has an update list that contains the tickers of the financial assets that we wish to get data on and the last and earliest record we have on that asset. If it is the first time when asking for data on a specific asset then we ask for all data available between 1970/1/1 up until the last trading day available. If the number of allowed calls for an API is greater then the number of assets to be updated, then all assets can be updated in a daily run. However, if it is not possible to update all assets in one update run, then we will update the rest of the assets in the following run so that we can get as close as we can to live data.
    \item \textbf{A run\_updates script} that starts an update round. It contains the configuration with the API names, the location of the access key (if it exists) and the number of calls to be done for each API.
    \item \textbf{An initialiser} module that is based on the persistent data directory which contains the asset classes and the assets in the form of CSV files. The initialiser gets a configuration similar to that given to the `run\_updates` script. Based on the asset classes, the assets and the APIs given, the initialiser creates an `update\_list` for each API.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{04Design/04Pictures/update_mechanism_flow_chart.png}
    \caption{Data Harvester Update Round \cite{TR}}
\end{figure}
To know more about how the updating mechanism works, please refer to \ref{dhav_logs}.

\subsubsection{API Handler}

For each API, the API Handler has a `call` method and a method that formats the data so that it fits our data model. Additionally, there is an `api\_selector` method that makes the right API call, based on what asset class is requested and a data frame format checker that does a final check before sending the data to the updating mechanism. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.47\textwidth]{04Design/04Pictures/api_handler_v3.png}
    \caption{API Handler Flow Chart \cite{TR}}
\end{figure}

\subsubsection{Standardization of Data}
All data written to the database has to be in the following format:

\begin{lstlisting}[language=Python, caption= Pandas Data Frame Format]
Columns: [AOpen<Decimal.decimal>, AClose<Decimal.decimal>,
          AHigh<Decimal.decimal>, ALow<Decimal.decimal>,
          IsInterpolated<Integer>] 
          
Index: [AssetTicker<String>, ADate <datetime.date>]
\end{lstlisting}

The data standardization is conducted as soon as the data frame has been received from the API. All APIs need to have a method that takes the data frame received and modifies is so that it abides by our standard. To further enforce our standard, we have also added a format checker in the API handler. It can be used to verify any newly added API or to catch any data frame modifications done by the API owners before they get written in our database.

\subsubsection{Interpolation}

Our business logic library requires continuous data in order to be able to perform its numerical analysis. In order to transform real-life data that includes weekends and bank holidays into a continuous data set, we have used interpolation. The type of interpolation used is nearest neighbour. To be more precise, the algorithm designed goes over the data in pairs of adjacent dates. If there are any missing days, then the older nearest neighbour is duplicated over the missing days. In addition to this, the duplicated rows are flagged as interpolated in the database. Here is a example of how it may look like:
\textbf{\newline Pre-Interpolation: }
\begin{center}
    \begin{tabular}{||c c c c c c c c||} 
        \hline
        Index & ADate & AHigh & ALow & AOpen &A Close&AssetTicker&IsInterpolated\\ [0.5ex] 
        \hline\hline
        3&2000-11-16&126.86&126.89&126.86&87.36&VFIAX&0 \\ 
        \hline
        4&2000-11-17&126.44&126.44&126.44&87.07&VFIAX&0\\
        \hline
        5&2000-11-20&124.12&124.15&124.15&85.47&VFIAX&0\\ 
        \hline
        6&2000-11-21&124.55&124.45&124.55&85.78&VFIAX&0\\
        \hline
    \end{tabular}
\end{center}

\textbf{\newline Post-Interpolation: }
\begin{center}
    \begin{tabular}{||c c c c c c c c||} 
        \hline
        Index & ADate & AHigh & ALow & AOpen &A Close&AssetTicker&IsInterpolated\\ [0.5ex] 
        \hline\hline
        3&2000-11-16&126.86&126.89&126.86&87.36&VFIAX&0 \\ 
        \hline
        4&2000-11-17&126.44&126.44&126.44&87.07&VFIAX&0\\
        \hline
        5&2000-11-18&126.44&126.44&126.44&87.07&VFIAX&1
        \\
        \hline
        6&2000-11-19&126.44&126.44&126.44&87.07&VFIAX&1 \\
        \hline
        7&2000-11-20&124.12&124.15&124.15&85.47&VFIAX&0\\ 
        \hline
        8&2000-11-21&124.55&124.45&124.55&85.78&VFIAX&0\\
        \hline
    \end{tabular}
\end{center}

\subsubsection{Maintenance Design}

All the financial assets that the Data Harvester will retrieve data for are stored in a folder that contains a CSV file for each financial asset class, where the asset class name is the name of that CSV file. Each row of a CSV file will contain the ticker, the last and earliest record and the full name of an asset. If there is no data on the asset in the database, then the last and earliest records are left empty so that the Data Harvester will pull all available information on the first update of that asset.\newline

\textbf{Logs\newline}
\label{dhav_logs}

An extensive logging mechanism that logs information from all running parts of the Data Harvester has been implemented. Logs should be consulted before and after making any modifications. All successful updating sessions have the following information:

\begin{enumerate}
    \item Date and time of when an update has started.
    \item The name of the API currently being updated and how many calls are going to be made.
    \item For each API call: 
        \begin{itemize}
        \item The asset class name, asset ticker and time range for the API call.
        \item The type of data frame retrieved.  It is $<$class `pandas.core.frame.DataFrame`$>$ if the API returned that the call worked.
        \item The time range in the data frame retrieved. The newest date always is the date of the last trading day for that asset.
        \item The shape of the data frame before the interpolation.
        \item The shape after the interpolation. For those assets that are traded every day, no new rows appear after the interpolation.
        \item A message that we started writing the data frame to the database.
    \end{itemize}
    \item A message that the requested number of API calls has been done.
    \item Repeated logs from point 2 if there are other APIs to be used or a log when the update has finished (i.e. if all APIs have been used).
\end{enumerate}

\textbf{Maintenance of the Assets:}

\begin{itemize}
    \item \textbf{Add/remove an asset class: }Create a new CSV file with the name of the asset class as a filename within the asset classes folder and add the columns in the following form:\newline
    \begin{lstlisting}
    Ticker,Last_Update,Earliest_Record,Name
    \end{lstlisting}
    It is important to remember to add the new asset class in the list of supported asset classes of the API that supports it.\newline
    To remove an asset class, delete the respective CSV file and re-run the initialization script so that the asset class will be removed from the future updates. 
    \item \textbf{Add/remove an asset: }Go into the asset class CSV file that contains the asset and add the required details under the correct columns.
    At the end of the CSV asset class file, add a line containing the ticker and name of the asset.\newline
    Example:
    \begin{lstlisting}
    Ticker,Last_Update,Earliest_Record,Name
    SPY,,,S&P 500
    \end{lstlisting}
    To delete an asset, you simply have to remove the row that contains it.
    
    \item \textbf{Re-add an asset: } First check in the database for the last and earliest records. Then introduce a new row that contains them.
\end{itemize}
\textbf{Maintenance of the APIs:}
\begin{itemize}
    \item \textbf{Adding a new API: \newline}
        The difficulty of adding a new API greatly varies. It depends on ease of API use for data access. As a general rule, the organisations that have financial incentives to provide functional APIs often have an easier integration process. On the other end, the governmental organisation or the NGOs that provide free data can have a lengthy implementation process.
        
        Here are the steps required for adding a new API.
        \begin{enumerate}
            \item Clearly outline the reason why you are adding the API. Is it for adding new assets that were not available before? Is it to replace an older API? Are you adding redundancy for a more reliable service?
            In any of those cases, the appropriate adding and removing of assets and asset classes is required.
            \item Add a wrapper to the API call so that it works with the updating mechanism. The updating mechanism passes an asset ticker, an asset class name and a time range for the dates it is requesting data for. In return, it expects a data frame with data from 1970-1-1 (in the case of assets that have been traded for a long time) or the first available date for newer ones. In the case that the API has no data on the requested ticker, the update mechanism expects single value 1. The call wrapper should have a name of the form `$<$api\_name$>$\_call`. 
            \item Create a method that formats the data frame retrieved by the API. Use the format checker method from the API Handler class to verify whether your formatting is correct. 
        \end{enumerate}
    
    \item \textbf{The API changes over time: }
    While it is hard to predict what will change, we know that if any changes happen with the APIs, it will be seen from the logs. 
    \begin{itemize}
        \item \textbf{Format changes:} If the format of the data retrieved changes, then the format checker combined with the API logs will be able to tell what is the exact new format and what has to be changed and what the maintainer has to do to fix the problem.
        \item \textbf{Network Errors:}  Depending on the error number, appropriate action should be taken. The network errors are match HTTP status codes.
        \item \textbf{Empty data frame:} It is easy to see if the data frame retrieved is of the right size by looking at the logs. In this case, there is likely a problem with the API and extra work is required to find a solution.
        \item \textbf{Malformed data:} This is hard to verify and unlikely to happen.  If an API is returning malformed data for all assets, then it should not be used for any further updates. If an API is working correctly but it provides bad data for one asset, then that asset should be removed. Depending on the combination of assets and API, one can check for malformed data by implementing cross-checks with multiple APIs for the same assets.
        \item \textbf{APIs Internal Errors:} Verify the latest documentation and perform the required modifications of the calling methods. There is a good chance that our code is using an outdated version of the API.
    \end{itemize}
\end{itemize}

\subsubsection{Security}
To identify possible vulnerabilities, we have created a scenario where an attacker is trying to disrupt our flow of data - or worse, gain control of our updating process. During this phase, we have identified three security concerns that will be listed below. \newline
\textbf{Security Concerns}
\begin{enumerate}
    \item \textbf{API calls: }The data received from an API can be intercepted and modified.
    \item \textbf{API keys: }The keys used by each API have to be stored safely.
    \item \textbf{Rogue API: }An API can send a malicious package instead of the data we are expecting.
\end{enumerate}

\textbf{Solutions}
\begin{enumerate}
    \item \textbf{API calls: }Depending on the API settings, we will use either an IPSec policy or a VPN connection. The details of this will be discussed with the financial data provider.
    \item \textbf{API keys: }We will salt and then hash the API keys with SHA512 before storing it. The number of financial data APIs on the market is low enough to allow us to use SHA512 without having to worry about computation time.
    \item \textbf{Rogue API: }We have added several checks that make sure that the data we retrieve from the APIs are what we expect it to be. If an API sends malformed data, then a critical error will be displayed and logs of what did the API sent will be saved for inspection. 
\end{enumerate}

\subsection{Database Structure}
\label{DB Structure}

\subsubsection{Data Segregation}

The decision was made early on to horizontally partition the data stored by Thalia into two parts - one consisting of data related to users and user accounts and the other of financial data related to asset classes, assets and their historical prices. The following is the list of reasons the team documented for this decision:

\begin{itemize}

\item One alternative revenue stream we identified early on was the sale of our financial data as a separate product. This process would be trivial if it was stored in a separate database. 
\item Although the security of both types of data is important to our business model, protecting user’s private information is the highest priority. The financial data is accessed by the Data Harvester, a separate program gathering data from many sources on the web and introducing additional security risks. Data segregation helps limit the scope of a potential data breach \cite{ciscoSeg}.
\item The two types of data serve two separate purposes. The modules responsible for managing each are also decoupled. Thus, separation helps to enforce the principle of least concern.
\item A large corpus of guides and examples on how to manage user accounts is available online. Extending any of these to include financial data might be difficult and risks leading to bad design.

\end{itemize}

The separation of dissimilar collections of data is a practice widely adopted in industry. Criteria for assessing when this approach is appropriate have also been documented \cite{dataSegImp}. Based on the decision to use SQLite as our DBMS and to maximize the portability and security of the financial data, we decided to use two separate databases.

\subsubsection{The Data Layer Module - Finda}

The Finda (for a definition see \ref{project_glossary}) module was designed to implement the data layer, acting as an intermediary between the Data Harvester / business logic and the financial data. It allows users to manage a number of databases implementing a common schema and give them access to a suite of tools for reading, writing and removing the data stored in each. In addition to this, the Finda module implements the following features:

\begin{itemize}
\item A system for managing user permissions to help reinforce separation of responsibilities among Thalia's other modules. 
\item Integrity checks to ensure the integrity of the data provided to the end user.
\item A suite of administrative features to aid with managing the application on the back end.
\end{itemize}

Users can access these features through an outwards facing virtual interface, designed based on the facade design pattern \cite{designPatterns}.

Finda's design was modeled after object relational mappers (ORMs), which are libraries offered by most popular web frameworks. Although the implementation of what is essentially our own ORM proved to be costly in terms of developer time, it allowed us to create a more focused module tailored to our requirements. This helped to streamline the development of other modules.

UML diagrams for the Finda module can be found in \ref{umls}.

\end{document}
