\documentclass[main.tex]{subfiles}

\begin{document}

\section{Coding and Integration}

After a brief overview of the project and the project plan, this section will focus on the main technologies used in the project and the rationale behind choosing them. Moving on, we will discuss how these components were integrated and eventually deployed.

\subsection{Overview}

One of the first decision we have made this term was to completely recreate our prototype of Thalia. Firstly, this radical move was the consequence of a new implementation decision (for more detail see \ref{Web Framework}). Secondly, as prototypes are meant to be disposable and are designed only to answer key questions about the system \cite{pragmaticprog}, the proof of concept served its purpose, giving us a chance to refine the structure and the quality of the code.

Despite the risks posed by using a Software Version Control (SVC) Host such as GitHub, we have decided to continue using it as our software development platform. The reasoning behind this builds on the argument developed in our Technical Report \cite{TR}, which highlights that our Data Processing Module is a completely separate component in our system which none of the other components is able to access. Additionally, we are storing API keys as environment variables in a file that is not tracked by our SVC system, which minimizes the probability of us exposing sensitive data.

We have also decided to develop the application with Python as our main choice of programming language. Even though this choice seemed obvious from the beginning, we did consider its main benefits, which are as follows:

\begin{itemize}
    \item Python is a high level programming language allowing us to better focus on the application.
    \item A standard choice for prototyping.
    \item Provides superb third party libraries and frameworks for free.
    \item Easy to integrate if we were to choose other languages at some point in our development.
    \item The whole team was already familiar with the language, saving us the precious time needed to learn another programming language.
    \item Our application does not require an unreasonable amount of computation, so there is no need for a more efficient programming language such as C \ref{BL} \cite{languagescomparison}.
\end{itemize}

We will discuss other technologies used in more detail after the discussion on project planning.

\subsection{Planning}

Early in the inception phase of development, we decided that our goal was not to have fixed responsibilities in our team, allowing everybody to work on each component of the system. This decision has also eased the code review process, as we had no status differences to distort the error-correcting mechanism \cite{statusdifference}. Furthermore, since no team member was the sole developer of a system component, this allowed us to direct comments at the code and not the author \cite{howtoreview}. For these reasons we have decided to follow the Egalitarian Team structure and make use of the flexibility offered by it.

Our workflow was centred around the tools provided by GitHub. We used a ticketing system to divide and distribute tasks among team members. These tickets were sometimes given by the team on the weekly meetings, but occasionally they were chosen by the member proposing the feature or change. Our goal with this approach was to divide larger jobs into smaller tasks.

A typical ticket in our project was an encapsulation of a user story, which consisted of a title, a one liner, value, acceptance criteria, and sub tasks. In the first half of development, we also used effort-oriented metrics (story points) to measure the amount of work incurred by taking on a ticket, but we decided to abandon this aspect. An example of a ticket can be seen on \figurename{\ref{Ticket}}.

\begin{figure}[H]
   \centering
   \includegraphics[scale=0.7]{05Coding/05Pictures/ticket.png}
   \caption{Ticket Example (source: https://github.com/)}
   \label{Ticket}
\end{figure}

Throughout the whole project, we had a total of 110 tickets, and 71 pull requests. The following graph also shows the number of contributions to master, excluding merge commits.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{05Coding/05Pictures/contributions.jpg}
   \caption{Contributions to the Master branch (source: https://github.com/)}
\end{figure}

Nevertheless, this rigour in creating tickets was not always achievable. In particular, it was difficult to create small enough tickets in the beginning of the development process when more fundamental components of the systems were developed. In these cases, we assigned the ticket to a pair or group of people. This approach achieved the following:

\begin{itemize}
    \item Improved the overall code quality and fastened production \cite{pairprogramming}.
    \item Minimised review time over the long run.
    \item Distributed the knowledge of large system components across a few people instead of one person.
    \item Eased introducing the new team member to the project.
\end{itemize}

In addition, we also had a scrum board as an overview for the ongoing tickets. Although the Scrum community engages in ongoing discussion about the benefits of a physical scrum board over an online one \cite{physicalscrum}, given no actual workplace, the former was not possible to achieve. The board allowed us to see which tickets needed to be reviewed and which were ready to be merged. The tickets/cards were distributed into columns, such as `To do`, `In progress`, `Review in progress`, `Review complete` and `Finished`. A truncated picture of this scrum board can be seen on figure \figurename{\ref{Scrum}}.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{05Coding/05Pictures/scrumboard.png}
   \caption{Scrum board - truncated}
   \label{Scrum}
\end{figure}

The workflow then largely depended on the task at hand and the person working on it. It was up to them if they met for pair-programming, or chose to work individually. In the beginning, we all agreed on the developing environment and coding standards (for a detailed analysis, see \ref{Coding Standards}). Since this report was a relevant portion of the workload, we decided to treat it as code. The report was written in \LaTeX with an overall structure of the document in a main.tex file which linked to individual sections. This let us to work on different sections separately, just like features in our software.

Regardless of the nature of the ticket, the week, or in case of some larger tasks, two weeks, ended by the creator indicating that the changes were ready to be integrated. This was done by publishing the changes and creating a new pull request, which - upon approval - pushed the changes onto the master branch. To ensure code quality, we set up a continuous integration (CI) environment (more on that in \ref{Coding Standards}). After all checks have passed, the changes were successfully integrated into the code base.

\subsection{Key Implementation Decisions}

Let us now discuss the main technologies used in our project. These can be categorised as follows:

\subsubsection{Web Framework}
\label{Web Framework}
Choosing the right web framework was more controversial than originally planned. The two major options for Python are Django and Flask. Although we decided to use Django for our MVP in the first semester, we had to spend a significant portion of the time available learning the framework, so we had to decide whether we were going to stick with it or learn Flask. In the end, we opted to go with Flask for the following reasons:

\begin{itemize}
    \item Django has one architecture that all projects must share, and we have designed the architecture for our project ourselves. While neither architecture is wrong, the two are not compatible. Flask, on the other hand, is structure-agnostic, so we can lay out the code as we see fit.

    \item Flask comes with the bare minimum for web-development, which means that we don't need to manage the complexity of any feature we're not using. Django has a more complete feature-set from the beginning. This would be desirable in a large web application, but introduces significant overhead in our case, where the website has only a handful of pages.

    \item Django all but insists on using its ORM for all database interaction, while we plan to have a more manual approach through the use of Finda.
\end{itemize}

Thus, we chose Flask, a Python-based micro web framework. Flask provides functionality for building web applications, such as managing HTTP requests, using the `requests` library, and rendering templates \cite{smyth_2018}. Flask leverages `Jinja2` as a template engine \cite{templates_2010}. We used the `render-template` library to process and parse the HTML templates for all the pages, except for the dashboard, which is described in more detail below. We used some of Flask's extensions, such as Flask-Login, which provides user session management for Flask \cite{flask-login}. To handle web forms we used the `flask-wtf` extension, which is a wrapper around the `WTForms` package \cite{wtforms-documentation}.

\subsubsection{GUI}
\label{GUI}

Dash is great for applications that require data visualisation, modelling, or analysis \cite{dash}, which is precisely what is needed for our portfolio backtesting software. It allows us to create reactive single-page apps, which means that with the use of tables, drop-down menus, and other forms of input, the app is updated instantly. This is done with the use of `callbacks` \cite{callbacks}. Callbacks allow us to exchange data with our web server asynchronously through AJAX requests whenever a user input changes. Upon receiving a response to such a request, the corresponding output is updated immediately without refreshing the page.

A typical callback using an example from our application can be seen below:

\begin{lstlisting}[language=Python, caption=setup.py - Development environment, label=lst:callback_example]
def register_add_portfolio_button(dashapp):
    dashapp.callback(
        Output("portfolios-container", "children"),
        [Input("add-portfolio-btn", "n_clicks")],
        [State("portfolios-container", "children")],
    )(add_portfolio)
\end{lstlisting}

In this code snippet, we register a listener to the button with the unique id: `add-portfolio-btn`, and its property `n\_clicks`, or number of clicks. Whenever the input property changes, the corresponding function gets called automatically. This also happens upon launching the Dash page. Therefore, it is common to start the called function by catering to this case and raising a `PreventUpdate` exception if the input parameter is `None`. An example can be seen in \cite{prevent_update}.

The called function then receives the input properties as parameters. Furthermore, we can add any number of parameters as `State` objects, as has been done above. When returning from a function call, the same number of values as is specified in the callback must be returned. Mismatching the number of return values results in a runtime error. Mismatching the input parameters, on the other hand, yields a different function signature, and so the callback is not registered with the desired function. Doing this does not raise any errors. 

Fortunately, Dash offers great developer tools \cite{dash_dev_tools}, such as the `Callback Graph` visible on \figurename{\ref{callback_graph}}, which can be accessed by running the application in debug mode and activating the Dev tools. This graph came in handy often, especially due to the arguments presented in \ref{dash_limitations}.

 \begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth,keepaspectratio]{05Coding/05Pictures/callback_graph.png}
   \caption{Callback Graph on Thalia}
   \label{callback_graph}
\end{figure}


\subsubsection*{Limitations}
\label{dash_limitations}

Although Dash is fairly intuitive and great for one-page applications, it has its limitations when it comes to dynamically creating components. To be more precise, all callbacks need to be registered before runtime. In our application the user is able to enter multiple portfolios via the `Add Portfolio` button. Doing so creates new input fields for the portfolio, which all need to be registered. 

As this problem persisted with other components as well, we needed to find a general solution. Many approaches have been suggested by the Dash Community, some even by the developer team \cite{dash_workaround}. In the end, we have decided to use the most stable approach and generate all the components beforehand and hide them until activated. This means that the relevant function now also has the responsibility of returning the visibility of the required div element. Note that this is a highly requested feature in the Dash community, and is likely to be added soon, but for now, we needed to resort to workarounds.

Furthermore, the data we show in the dashboard is greatly centered around the strategy object discussed in \ref{BL Structure}. At the beginning of every backtesting process, a new strategy object is initialised, which is a time-costly task. As this object holds most of the information our result pages require, many functions would need to access the same object. 

One solution would be to have a callback for initialising the object, and then share it with all others. Although there exist ways to do so \cite{dash_share}, we were not satisfied with any of the approaches listed. Consequently, we decided to go forward with the original approach that Dash favours, and combine more outputs in one function. Even though this introduced more complexity to our code, it made the application faster overall.

\subsubsection*{Layout}

The layout of the application, as discussed in \ref{GUI}, has been created with the use of the `Dash HTML components` library. Consequently, there was no need to write HTML for the dashboard, since the layout was composed using Python, which then generates HTML content. Dash uses the Flask web framework, which makes it fairly simple to embed a Dash app at a specific route of an existing Flask app \cite{flask-dash}. The framework also handles most of the communications between the front-end and back-end.

The User-Interface was made with the use of `Bulma`, which is an open-source CSS framework \cite{bulma}. Bulma is ideal for creating responsive elements, as it automatically adapts the website to different screen sizes \cite{responsiveness_2020}. It is also compatible with various web browsers. Another advantage of Bulma is that we could create a very modern-looking website quickly since the framework is doing a lot of the heavy lifting for us.

It also provides pre-styled elements, components (such as the navigation bar) and alerts for the users. Additionally, it is intuitive to learn when compared to `Bootstrap`, as Bulma uses memorable class names, such as `.button`, and has a very straightforward modifier system. It consists of nothing but CSS stylesheets without any use of JavaScript, which makes it very compatible with a framework like Dash.

\subsubsection{Business Logic}
\label{BL}

The business logic module sits at the very core of our application. We require it for producing a time series of a portfolio's performance as well as selected key risk metrics of a given asset allocation. This output is consumed by our web application and presented in plots and tables as described in \ref{GUI}.

Research into developing this module was initiated by listing our requirements for its desired behaviour. We identified that it should support:

\begin{itemize}
    \item Specification of a portfolio as a set of pandas dataframes, the common data exchange format in our application.
    \item Calculation of a portfolio's return over time.
    \item Regular contributions to mimic saving.
    \item Rebalancing strategies to reestablish the desired weighting of a portfolio.
    \item Calculation of key metrics, including the Sharpe and Sortino Ratio, Max Drawdown, Best and Worst Year.
    \item Collecting and reinvesting dividends for equities.
\end{itemize}

Using these requirements, we struggled to find an open-source library that would handle these tasks for us. While backtesting libraries written in Python are available in abundance (e.g. PyAlgoTrade \cite{PyAlgoTrade} and bt \cite{bt}), each of these was lacking in at least one critical aspect. None of them support specifying a portfolio using absolute or relative weights and instead seem to focus on backtesting trading strategies involving just a single asset while using technical indicators. Thus, we made the decision to develop our own library for handling the aforementioned tasks.

The result of this effort is `Anda`. For each of its functions, Anda takes as input a `Strategy` object that specifies the entire list of parameters for a backtest, including contribution dates, a list of assets with associated price data, dividends for equities, etc. Calculations are performed on a per-metric basis by separating them into individual functions.
This approach has allowed us to tailor the entire business logic module exactly to our needs without having to produce complicated wrapper code for existing backtesting libraries.

\subsubsection{Database and Finda}
\label{Finda}

Another major technology decision was the choice of appropriate database management system (DBMS) for storing historical price data collected by the data harvester. Before committing to a specific technology, we identified the following requirements a suitable DBMS should fulfil:

\begin {itemize}
    \item \textbf{Schema:} The structure of our data is relatively simple. Consequently, Thalia does not require support for sophisticated features and data types. A suitable DBMS should be able to accommodate the database schema designed last term, with the addition of simple integrity constraints and cascade operations.
    \item \textbf{Support:} Ideally, the DBMS should be cross platform, as this would allow us to defer commitment to a specific deployment platform until we are ready to start the CD process. 
    \item \textbf{License and pricing:} The DBMS should be free to use and have a non-restrictive license.
    \item \textbf{Performance:} The DBMS should be able to handle a high volume of concurrent reads to fulfill user requests. The data will be updated daily, meaning efficient write operations are a lower priority.
    \item \textbf{Usability:} As our team lacks experience in this field, a suitable DBMS should be relatively simple to learn. Ideally, team members should be able to learn the basics in a single week long sprint.
    \item \textbf{Security:} The DBMS should have a mature code base and be relatively secure, as access to financial data is a key component of our business model. Later, it will likely also store data that is not available through public APIs, meaning potential data breaches could expose us to legal liability. \cite{dataprotectionGov}
    \item \textbf{Type:} Since the project constraints specify we use SQL queries, only relational DBMSs supporting a version of SQL are appropriate.
\end{itemize}

MySQL, PostgreSQL, SQLite and MariaDB were subject to in-depth comparison based on fulfilment of the above requirements and industry adoption \cite{dbPerfComparison} \cite{dbmsMarketShare}. Our final decision was to use SQLite for the following reasons:
\begin{itemize}
    \item It is user-friendly and easy to deploy, allowing us to start continuous deployment faster.
    \item It has a small footprint and offers good performance \cite{dbPerfComparison}.
    \item Portable serverless design aids with development and testing.
    \item All team members have experience working with SQLite from previous term. This helps to reduce the overhead of knowledge transfer.
\end{itemize}

The main drawbacks of using SQLite, namely scalability and performance, are not a concern at this stage, as the current version of Thalia is meant to be a high-quality industrial prototype. As such, it will not contain the full range of financial data needed for marketability. Should SQLite prove to be inadequate in the future, we would be able to switch to a different DBMS with relatively little trouble, as the process of database migration is exceedingly well-documented \cite{frameworkDataMigration} \cite{understandingDataMigration}. To pre-empt any difficulties that might arise, the decision was made to design the data layer to easily accommodate such a migration.

\subsubsection{Data Harvester}
\label{Data Harvester}

The role of the Data Harvester is the collection of live data from multiple source, parsing it to a format that makes it usable by Anda and easily storable by our in house DBMS system Finda. In addition to this, a mechanism for changing the data collected and adapting to the market demands has to exist. Another requirement relates to the limitations that are imposed by the third-party APIs we use. The first design decision made in order to accomplish the above was the creation of the updating mechanism.

All financial backtesting applications rely on having accurate data to work with. We required a system that can aggregate the data from multiple APIs and that would keep our database updated with live data.
The requirements for such a system were:

\begin{itemize}
    \item \textbf{Standard data format:} To be able to store the data into a database with Finda and have the financial analysis done by Anda, the Harvester needs to parse all the data received through the APIs to a standard format without losing numerical accuracy or granularity.
    \item \textbf{Update the database:} The database has to be updated regularly to contain live data.
    \item \textbf{Data Interpolation:} Some financial assets require interpolation to account for days without any transactions (e.g. weekends and public holidays).
    \item \textbf{Redundancy:} We have no power over the APIs we are using. The required system needs to have redundancy measures so that we can retrieve live data even if an API fails.
    \item \textbf{Maintenance:} The financial world is in a continuous change. Because of this, it is important to make sure that we can add and delete assets as time goes on.
\end{itemize}

\subsubsection{Continuous Updating Mechanism}
The Updating consists of three components:
\begin{itemize}
    \item \textbf{The update procedure} made out of a series of methods. The procedure starts by looking at what APIs are available to be called. Each API has an update list that contains the tickers of the financial assets that we wish to get data on and the last and earliest record we have on that asset. If it is the first time when asking for data on a specific asset then we ask for all data available between 1970/1/1 up until the last trading day available. If the number of allowed calls for an API is greater then the number of assets to be updated, then all assets can be updated in a daily run. However, if it is not possible to update all assets in one update run, then we will update the rest of the assets in the following run so that we can get as close as we can to live data.
    \item \textbf{A run\_updates script} that starts an update round. It contains the configuration with the API names, the location of the access key (if it exists) and the number of calls to be done for each API.
    \item \textbf{An initialiser} module that is based on the persistent data directory which contains the asset classes and the assets in the form of CSV files. The initialiser gets a configuration similar to that given to the `run\_updates` script. Based on the asset classes, the assets and the APIs given, the initialiser creates an `update\_list` for each API.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{04Design/04Pictures/update_mechanism_flow_chart.png}
    \caption{Data Harvester Update Round \cite{TR}}
\end{figure}
To know more about how the updating mechanism works, please refer to \ref{dhav_logs}.

\subsubsection{API Handler}

For each API, the API Handler has a `call` method and a method that formats the data so that it fits our data model. Additionally, there is an `api\_selector` method that makes the right API call, based on what asset class is requested and a data frame format checker that does a final check before sending the data to the updating mechanism. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.47\textwidth]{04Design/04Pictures/api_handler_v3.png}
    \caption{API Handler Flow Chart \cite{TR}}
\end{figure}

\subsubsection{Standardization of Data}
All data written to the database has to be in the following format:

\begin{lstlisting}[language=Python, caption= Pandas Data Frame Format]
Columns: [AOpen<Decimal.decimal>, AClose<Decimal.decimal>,
          AHigh<Decimal.decimal>, ALow<Decimal.decimal>,
          IsInterpolated<Integer>] 
          
Index: [AssetTicker<String>, ADate <datetime.date>]
\end{lstlisting}

The data standardization is conducted as soon as the data frame has been received from the API. All APIs need to have a method that takes the data frame received and modifies is so that it abides by our standard. To further enforce our standard, we have also added a format checker in the API handler. It can be used to verify any newly added API or to catch any data frame modifications done by the API owners before they get written in our database.

\subsubsection{Interpolation}

Our business logic library requires continuous data in order to be able to perform its numerical analysis. In order to transform real-life data that includes weekends and bank holidays into a continuous data set, we have used interpolation. The type of interpolation used is nearest neighbour. To be more precise, the algorithm designed goes over the data in pairs of adjacent dates. If there are any missing days, then the older nearest neighbour is duplicated over the missing days. In addition to this, the duplicated rows are flagged as interpolated in the database. Here is a example of how it may look like:
\textbf{\newline Pre-Interpolation: }
\begin{center}
    \begin{tabular}{||c c c c c c c c||} 
        \hline
        Index & ADate & AHigh & ALow & AOpen &A Close&AssetTicker&IsInterpolated\\ [0.5ex] 
        \hline\hline
        3&2000-11-16&126.86&126.89&126.86&87.36&VFIAX&0 \\ 
        \hline
        4&2000-11-17&126.44&126.44&126.44&87.07&VFIAX&0\\
        \hline
        5&2000-11-20&124.12&124.15&124.15&85.47&VFIAX&0\\ 
        \hline
        6&2000-11-21&124.55&124.45&124.55&85.78&VFIAX&0\\
        \hline
    \end{tabular}
\end{center}

\textbf{\newline Post-Interpolation: }
\begin{center}
    \begin{tabular}{||c c c c c c c c||} 
        \hline
        Index & ADate & AHigh & ALow & AOpen &A Close&AssetTicker&IsInterpolated\\ [0.5ex] 
        \hline\hline
        3&2000-11-16&126.86&126.89&126.86&87.36&VFIAX&0 \\ 
        \hline
        4&2000-11-17&126.44&126.44&126.44&87.07&VFIAX&0\\
        \hline
        5&2000-11-18&126.44&126.44&126.44&87.07&VFIAX&1
        \\
        \hline
        6&2000-11-19&126.44&126.44&126.44&87.07&VFIAX&1 \\
        \hline
        7&2000-11-20&124.12&124.15&124.15&85.47&VFIAX&0\\ 
        \hline
        8&2000-11-21&124.55&124.45&124.55&85.78&VFIAX&0\\
        \hline
    \end{tabular}
\end{center}

\subsubsection{Security}
To identify possible vulnerabilities, we have created a scenario where an attacker is trying to disrupt our flow of data - or worse, gain control of our updating process. During this phase, we have identified three security concerns that will be listed below. \newline
\textbf{Security Concerns}
\begin{enumerate}
    \item \textbf{API calls: }The data received from an API can be intercepted and modified.
    \item \textbf{API keys: }The keys used by each API have to be stored safely.
    \item \textbf{Rogue API: }An API can send a malicious package instead of the data we are expecting.
\end{enumerate}

\textbf{Solutions}
\begin{enumerate}
    \item \textbf{API calls: }Depending on the API settings, we will use either an IPSec policy or a VPN connection. The details of this will be discussed with the financial data provider.
    \item \textbf{API keys: }We will salt and then hash the API keys with SHA512 before storing it. The number of financial data APIs on the market is low enough to allow us to use SHA512 without having to worry about computation time.
    \item \textbf{Rogue API: }We have added several checks that make sure that the data we retrieve from the APIs are what we expect it to be. If an API sends malformed data, then a critical error will be displayed and logs of what did the API sent will be saved for inspection. 
\end{enumerate}

\subsection{Integration and Deployment}
\label{Coding Standards}

 Writing well documented and good quality code is one thing, but making sure it all works together as a whole is a completely different story. In the first term, many hours have been wasted on trying to integrate different components of the system which did not want to fit together. Even then, we had some DevOps tools in place \cite{DevOps}, but considering that we had to produce significantly less code and more documentation last term, this was not a priority.

 Starting afresh the coming term, we have decided to set up the development environment again. Upon opening the `setup.py` file, we see a list of required libraries, and the following lines of code:

\begin{lstlisting}[language=Python, caption=setup.py - Development environment, label=lst:Development_env]
"""A setuptools based setup module."""
from os import path

from setuptools import find_packages, setup

here = path.abspath(path.dirname(__file__))

install_requires = [
    "flask",
    "flask-login >= 0.5",
    "flask-migrate",
    "flask-wtf",
    "pandas",
    "dash",
]


tests_require = ["pytest", "coverage"]

extras_require = {"dev": ["black", "flake8", "pre-commit"], "test": tests_require}

setup(
    name="Thalia",
    version="0.2.0",
    packages=find_packages(),
    install_requires=install_requires,
    extras_require=extras_require,
)
\end{lstlisting}

One of the first decisions we had to make was to choose a standard coding style. This is exactly what `flake8` is for, which we can see amongst the extras in the code snippet above. The original documentation of flake8 defines it as ``[...] a command-line utility for enforcing style consistency across Python projects. By default it includes lint checks provided by the PyFlakes project, PEP-0008 inspired style checks provided by the PyCodeStyle project, and McCabe complexity checking provided by the McCabe project`` \cite{flake8}. However, as many other developers, we also decided to redefine the maximum line-length from 79 to 88 as we found this convention to be a hindrance when writing code.

Another tool used for enforcing uniform style was the `black` auto-formatter for Python \cite{black}, which formatted the code for us upon every save (if enabled) and when committing code to GitHub. This has been achieved by the use of githooks \cite{githooks}, which are programs that are triggered upon certain git actions. Setting up these hooks required pre-commit package. This ensured that both flake8 and black were guaranteed to run before publishing changes.

\subsubsection{Continuous Integration}
\label{Continuous Integration}

Many studies have investigated the positive effects of developing in a continuous integration (CI) environment (see, for example, \cite{CI_1} and \cite{CI_2}). Regardless of the exact implementation, its obvious benefit is that it provides security and uniformity for projects. We already made some steps to achieve a uniform style, but had no means to know whether the code published has been also passed its tests. Note that, in this section, we will only discuss testing as a part of CI and not the testing strategy (for this, see \ref{Testing}).

Another important part of the DevOps toolchain is the use of containers, which is what Docker helps to achieve \cite{Docker}. Docker helps developers focus on writing code rather than worrying about the system the application will be running on and also helps to reveal dependency and library issues. As Docker is open source, there are many free to use docker images available online \cite{DockerImages}. When choosing the CI environment, Docker support was one of the main requirements.

The most promising candidate for this was CircleCI \cite{CircleCI}, which is a cloud-based system with first-class Docker support and a free trial. After connecting our GitHub repository to CircleCI and setting up a configuration, CircleCI now does the following on every pull-request:

\begin{enumerate}
    \item Sets up a clean Docker container or virtual machine for the code.
    \item Checks previously cached requirements (for more detail see \figurename{\ref{cache_CI}}).
    \item Installs the requirements from `requirements.txt`
    \item Caches the requirements for faster future performance.
    \item Clones the branch needed to be merged.
    \item Runs flake8 one last time and saves results.
    \item Runs tests and saves results.
    \item Deploys the master branch to AWS, see \ref{Continuous Deployment}
\end{enumerate}

 \begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{05Coding/05Pictures/cache_CI.png}
   \caption{CircleCI on Caching (source: https://circleci.com/docs/2.0/caching/)}
   \label{cache_CI}
\end{figure}

The outcome of these steps is visible on the CircleCI web platform, but - more importantly - also on GitHub. CircleCI will prevent a merge with master if failing tests (or no tests) have been found.

 \begin{figure}[H]
   \centering
   \includegraphics[scale=0.6]{05Coding/05Pictures/circleCI.png}
   \caption{CircleCI on GitHub (source: https://github.com/)}
   \label{CircleCI}
\end{figure}

With these steps, we managed to ensure that the code written is uniform and of good quality. It significantly reduced the time needed for integrating and code reviewing. The last step was now to deploy the system.

\subsubsection{Hosting and Continuous Deployment}
\label{Continuous Deployment}

An overview over the literature covering deployment reveals a plethora of different strategies for deploying and hosting web applicaitons \cite{ConnollyFundamentals}. Our decision of how to choose among them involved the following considerations:

\begin{itemize}
    \item Price - since we have severe budget constraints, we were looking for a cheap hosting solution
    \item CircleCI support - The target host should be supported by CircleCI natively to ease development of a continuous deployment (CD) pipeline
\end{itemize}

The upfront cost of buying physical servers ruled it out as an option for us. Thus, we turned our attention to using a solution that involved deployment to a virtual machine in the Cloud.

Due to native CircleCI support and a free-tier service, we initially chose Heroku \cite{Heroku} as our initial hosting provider. This allowed us to host our application for free in the initial stages of development while providing ample opportunity for horizontal and vertical scaling later on, if required.

However, it quickly became apparent that Heroku was not appropriate for our purposes, due to its lack of support for sqlite databases \cite{HerokuSqlite}. Hence, we moved to AWS \cite{AWS}, which is also supported by CircleCI \cite{AWSCircleCI}.

The benefits of using continuous deployment have been well established for multiple years and involve ``the ability to get faster feedback, the ability to deploy more often to keep customers satisfied, and improved quality and productivity`` \cite{CDBenefits}. Using AWS in combination with CircleCI, our CD pipeline involves the following simple steps:

\begin{enumerate}
    \item Upon commits to the master branch on GitHub, CircleCI triggers a workflow.
    \item The workflow first executes the steps listed in \ref{Continuous Integration} to ensure the validity of the current codebase state.
    \item If this step is successful, the master branch is pushed to a remote repository recognized by AWS via git.
    \item AWS executes the `Procfile` script stored in the root of our project to start the application using a `gunicorn` web server \cite{Gunicorn}.
\end{enumerate}

Our deployment process is thus fully automated and immune to failing tests, as it will only complete successfully if the application is in a correct state. The full CI/CD workflow is captured by the flowchart on \figurename{\ref{CI_CD}}.

 \begin{figure}[H]
   \centering
   \includegraphics[scale=0.6]{05Coding/05Pictures/CI_CD.png}
   \caption{CI/CD workflow}
   \label{CI_CD}
\end{figure}

\end{document}
